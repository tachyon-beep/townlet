# Townlet Target Architecture & Migration Plan

Current Challenges and Rationale
Townlet’s current implementation has several architectural pain points that motivate an overhaul before v1.0:
• Monolithic Core Modules – Key subsystems are implemented as giant “god object” modules (e.g. world/grid.py ~2,535 LOC, policy/runner.py ~1,493 LOC, telemetry/publisher.py ~2,190 LOC). These massive files bundle multiple concerns, creating tight coupling that hinders maintainability and evolution[1]. For example, the world engine module manages everything from agent state to console I/O in one place, making it hard to test or swap out parts[2]. Similarly, the policy runner assumes heavy ML logic inline and the telemetry publisher mixes authentication, event processing, and threading in one module[3][4].
• Lax Quality Gates – The codebase currently fails many quality checks. Ruff linter flags ~180 violations, mypy reports ~474 type errors, and docstring coverage is only ~29.9%[1]. These indicators show significant maintainability debt and lack of enforcement of standards. The project lacks continuous integration enforcement, so issues accumulate unchecked.
• Fragile Dependency Management – Optional dependencies (like PyTorch torch for ML or httpx for HTTP) are not handled gracefully. In a fresh install, running tests triggers ~50 errors because those libraries are missing[1]. There are no stubs or fallbacks for absent optional packages, so the default environment cannot even execute the full test suite[5]. This impedes contributors who don’t have every heavy dependency installed and complicates CI/CD.
Overall, the high-level design is solid in theory but only partially realized in code – the current state was graded “C” in a recent audit[6]. To address these issues, we propose a new aspirational architecture that emphasizes modularization, clear interface boundaries, and improved testability/maintainability. Since the project is pre-1.0 (with no backward compatibility constraints or external stakeholders), we have the freedom to make sweeping changes now to set a strong foundation for the future.
Target Architecture Overview (Aspirational Final State)
The target architecture retains the existing simulation loop as the central orchestrator but refactors each major subsystem behind well-defined interfaces. The simulation loop will interact with abstract interfaces (protocols) for World state, Policy logic, and Telemetry, instead of directly instantiating concrete classes[7]. Factories or registries will provide the actual implementations based on configuration at runtime, enabling pluggability and easier testing[7].
+---------------------+
|  Simulation Loop    |
|  (Orchestrator)     |
+----------+----------+
           | interfaces
           v
+----------+----------+----------------------+---------------------+
| World API| Policy API| Telemetry API        | Ancillary Services  |
+----------+----------+----------------------+---------------------+
| World    | Policy    | Telemetry            | Rewards, Scheduler, |
| Packages | Backends  | Pipelines            | Stability, etc.     |
+----------+----------+----------------------+---------------------+
Layered Architecture: At a high level, the system will be organized in layered components with clear contracts:
• Simulation Loop (Core Orchestrator) – Manages the tick cycle, advancing time and coordinating subsystems each tick (world state updates, policy decisions, reward calculation, etc.). Importantly, the loop will call into subsystems only via three primary interfaces[8]: a WorldRuntime interface for world state queries & state mutation, a PolicyBackend interface for agent decision logic, and a TelemetrySink interface for metrics/events output. The loop no longer imports concrete implementations of these, instead using a factory (e.g. WorldFactory, PolicyFactory, etc.) to retrieve the appropriate implementation based on config at startup[9][10]. This inversion of control allows us to swap implementations (e.g., use a stub world or a different policy AI) without changing the loop code, facilitating testing and extension.
• Domain Subsystems (World, Policy, Telemetry, etc.) – Each major subsystem is refactored into a modular, cohesive package hiding behind its interface. Rather than one giant module per subsystem, we will have multiple smaller modules organized by domain concerns[11]. For example, the world/ package will be split into logical subpackages: agents and relationships, affordances and hooks, console and narration, employment, relationships, simulation_state (world state management), etc[11][12]. The world subsystem will expose a clean façade (e.g. a WorldContext implementing WorldRuntime) that provides only the methods needed by other parts of the system, insulating its internal complexity[13]. The policy subsystem will separate the high-level orchestration from specific backend implementations – e.g. a simple scripted policy vs. a PyTorch-based RL policy – all conforming to the PolicyBackend interface[14]. The telemetry subsystem will be broken into a pipeline with distinct components for collecting data, transforming/formatting it, and transporting or displaying it[15]. These pieces (e.g. metric aggregators, event transformers, transport adapters for output channels like stdout, file, WebSocket) will all plug into a unified TelemetrySink service with proper lifecycle management [startup/shutdown of background threads](16). Critically, ancillary services like rewards calculation, scheduling events, or stability checks will also be refactored to depend only on the abstract world interface [not on concrete world.Grid internals](17). This ensures even smaller services remain loosely coupled and easily testable.
• Infrastructure and Configuration Layer – The configuration system and dependency injection mechanism will support the above modular design. The config loader will be refactored to map config options to factory names or plugin identifiers rather than hard-coded module paths[10]. This means a config can specify which implementation of PolicyBackend to use (e.g. “scripted” vs “pytorch”) and the system will load the corresponding class via a factory or registry. We will leverage Python entry points or plugin registries for optional components – for instance, an optional FastAPI-based admin web UI or a snapshot exporter can register itself so that the core system discovers it if enabled[10]. Dependency injection (either through factories or a simple service container) will occur at startup, constructing all needed subsystem instances and passing them to the simulation loop, rather than the loop instantiating things ad-hoc. This yields a cleaner initialization sequence and makes it possible to swap components in different environments (test vs production) easily.
• Optional Dependencies & Extensibility – To solve the current optional dependency chaos, the new architecture introduces explicit optional modules and graceful degradation for missing extras. In the Python packaging, we will define extra feature groups in pyproject.toml [for example, an [ml] extra for machine-learning features like the PyTorch policy, an [api] extra for web API/UI components, etc.](18). The factories for subsystems will check if the required libraries are available (and if the feature is enabled in config) before instantiating those components[19]. If a dependency is not installed, the factory will return a stub implementation that satisfies the interface but simply logs a warning or provides a no-op behavior[20]. This way, the simulation can continue running with reduced functionality rather than crash. For example, if torch is absent but the config requests a PyTorch policy, the system might fall back to a simple scripted policy or a stub that informs the user that ML is unavailable[18]. Tests will be written to assert that stubs are used when optional deps are missing, ensuring our CI can run a “minimal environment” (no heavy extras) and still pass all tests[21]. This strategy greatly improves approachability and CI stability, since contributors can run a basic simulation and test suite without needing every optional component installed.
• Quality Gates and Observability – As part of the target state, we plan to enforce much stricter quality checks and improve observability hooks. The refactored code will integrate linting, type checking, and doc coverage into the development workflow (and CI) so that the earlier 180+ lint errors and 474 type errors are driven down and kept down over time[22]. Each module will be easier to document and test in isolation, so we expect documentation coverage and testing to dramatically improve. Telemetry being modular means we can add observability backends easily – e.g. plugging in a Prometheus metrics exporter or live web dashboard becomes feasible without core changes. The new TelemetrySink design, with explicit startup/shutdown control, ensures that background threads or async tasks for metrics and logging are well-behaved and don’t impede clean shutdowns[16]. Overall, these improvements will make the system more transparent and maintainable.
Expected Benefits: Adopting this modular, interface-driven architecture yields multiple benefits:
• Loose Coupling & Testability – Clear boundaries (WorldRuntime, PolicyBackend, TelemetrySink) mean each part can be developed and tested independently. We can swap in fake or simplified implementations (e.g. a dummy world or a stub policy) for unit tests, enabling targeted testing and faster iteration[23]. Refactors are safer because the impact is localized behind interface contracts[24].
• Improved Maintainability – No more giant god-classes; code is organized into coherent modules matching the domain concepts. This alignment with the high-level design makes the code easier to understand, and developers can work on separate subsystems in parallel with minimal merge conflicts[25]. The enforcement of linting/typing will catch issues early, and a higher docstring coverage (aiming well above the current ~30%) will aid new contributors.
• Optional Feature Flexibility – With explicit optional components and stubs, the core simulation can run in a minimal environment, lowering the barrier to entry for new users or contributors[26]. Advanced features (ML policies, web UI, etc.) become plug-and-play – if you need them you install the extra, if not the system degrades gracefully. This also ensures CI reliability: a baseline CI run can exclude heavy extras and still fully pass[27].
• Better Observability & Extensibility – A modular telemetry pipeline means we can extend outputs easily (e.g. adding a new logging backend or metrics sink without touching core logic). Explicit lifecycle management for background services (telemetry threads, etc.) improves stability in long-running simulations. In short, the system becomes more transparent, debuggable, and easier to evolve in the future[24].
With the target architecture defined, the next step is to implement it through a phased refactoring plan. Below is a migration roadmap breaking down the transformation into manageable work packages.
Migration Plan (Work Packages)
The migration to the new architecture will be executed in a sequence of work packages, each delivering part of the refactor while keeping the project functional. We will prioritize establishing good infrastructure (tooling, interfaces) first, then modularizing each subsystem. No strict timeline is imposed (solo open-source effort), but the logical order of steps is as follows:
Work Package 0: Enable Reliable Tooling Baseline
Objective: Establish a dependable development/test baseline so that we can refactor safely. Currently, missing optional dependencies and lax CI cause test failures – this package fixes that foundationally[1].
Key Activities: - Introduce optional dependency extras in the packaging (e.g. define [ml] for ML-related deps like PyTorch, [api] for web API/UI deps). Update import logic to guard optional imports so that if a library isn’t installed, the module won’t crash but will either skip or use a stub[28]. - Document how to do a minimal installation (which extras to omit for a lightweight setup). Set up CI jobs that run the linters (ruff), type checker (mypy), docstring coverage (interrogate), and a subset of tests without optional extras[28]. This ensures the core package is sound on its own. - Implement pytest skip markers for tests that require optional components[28]. For example, mark ML-related test cases to skip if [ml] extra isn’t installed, rather than erroring out. This will eliminate the 50+ test collection errors and yield a green test suite on a base install.
Dependencies: None. This is the first step to get the house in order.
Exit Criteria: All core tooling checks pass reliably in CI with only the base dependencies. Running pytest in a default environment (no extras) should result in 0 failures [skips for missing extras are okay](29). Likewise, lint and type checks should run clean or at least be enforced to a baseline so we can improve them incrementally.
Work Package 1: Define Core Interfaces and Factories
Objective: Introduce abstraction boundaries so the simulation loop no longer depends on concrete implementations of world, policy, or telemetry. We create the interface layer that underpins the new architecture[30].
Key Activities: - Define protocol or abstract base classes for the primary interfaces: WorldRuntime (for world state & operations), PolicyBackend (for decision making logic), and TelemetryService [for telemetry sinks](31). These should specify the minimal methods/events the simulation loop needs to call. We might use Python’s abc.ABC or typing.Protocol to define these interfaces. - Implement factory functions or a registry that maps a config choice to a concrete class instance[32]. For example, a WorldFactory.create(config) that reads the config (or uses default) and instantiates the appropriate WorldRuntime implementation (perhaps the existing World class wrapped to fit the interface). - Refactor the simulation loop initialization to use these factories. The loop should request, say, a world instance via the factory instead of doing WorldGrid() directly. Update any simple “smoke test” or bootstrapping code to pass in stub implementations for these interfaces to verify the wiring works[33].
Dependencies: WP0 should be done (we need the testing framework stable and linting in place to safely refactor).
Exit Criteria: The simulation loop and other high-level orchestrator code import only the interface types (protocols) and factory functions, not the concrete classes[34]. We should be able to swap in a dummy implementation of WorldRuntime or PolicyBackend in a test context via the factory – confirming that our abstraction layer is working. All tests should pass using the real implementations via factories.
Work Package 2: Modularize World Subsystem
Objective: Break apart the monolithic world module (world/grid.py) into a well-structured package. The world subsystem will be divided into cohesive subcomponents aligned with domain concepts, without altering external behavior[35].
Key Activities: - Identify logical clusters of functionality in the world code [e.g. agent management, spatial grid and simulation core, console command handling, employment system, relationships between agents, hookable events](36). Create new modules or subpackages under world/ for each cluster – for instance, world/agents.py, world/affordances.py, world/console.py, world/employment.py, world/relationships.py, etc., as needed to separate concerns[13]. - Implement a WorldContext façade (or similar) that implements the WorldRuntime interface from WP1[13]. This class will hold instances of the above subcomponents or orchestrate their interaction, but keep its public methods focused on the interface (e.g. methods to query state, apply an action, get a snapshot). The existing Grid or world class logic will be refactored such that most of its code moves into the submodules, and WorldContext just coordinates them. - Update other parts of the system (policy, telemetry, etc.) to use the new WorldRuntime interface instead of reaching into world.grid internals. This might involve adjusting imports to point to the facade or passing the facade into those components. - Add unit tests for the new world submodules. For example, if agent logic is now in world/agents.py, write tests for agent creation, employment, etc. Also add high-level regression tests that run a few simulation ticks to ensure the refactored world produces the same outcomes as before (to catch any behavioral regressions).
Dependencies: WP1 should be completed, so that consumers (like policy or telemetry) have been switched to interface-based access. This way, as we change the world internals, the rest of the code is calling through WorldRuntime and won’t break as long as we preserve the interface contract[37].
Exit Criteria: The world/grid.py file, which was ~2.5k lines, should shrink dramatically (e.g. <500 LOC) and serve mostly as glue or backward-compatibility shim[38]. All world-related functionality should now live in well-named modules. External code (policy, etc.) should import WorldRuntime or WorldContext instead of world.grid. Tests covering world behaviors should all pass, and ideally we have increased test coverage on world logic due to new unit tests.
Work Package 3: Refactor Telemetry Pipeline
Objective: Rework the telemetry subsystem (telemetry/publisher.py and related classes) into a set of composable services with a clear interface. This addresses the current telemetry module’s bloat (2k+ LOC) and unclear responsibilities[4].
Key Activities: - Decompose telemetry into distinct components that implement the TelemetryService (or TelemetrySink) interface defined in WP1[39]. Likely pieces include: a transport layer (responsible for sending out data via some channel or protocol, e.g. WebSocket server, file logging, stdout printing, etc.), an aggregation layer (collecting and structuring events/metrics from the simulation), and perhaps an authentication/authorization module if there are console access controls. - Implement proper lifecycle management for telemetry background processes. For example, if the telemetry involves threads or asyncio tasks (for streaming outputs or periodic metrics), provide context manager or startup/shutdown methods in the Telemetry interface to manage these[40]. The simulation loop can call a telemetry.start() at launch and telemetry.stop() on shutdown to gracefully handle threads. - Provide basic telemetry adapters for common outputs: at least a console/STDOUT logger and a file logger [so users can get logs without additional infrastructure](40). Design the transport adapter system such that adding a new adapter (say, an HTTP POST client or a Prometheus exporter) is straightforward via the Telemetry interface without modifying core logic. - Increase test coverage by writing unit tests for each telemetry component (e.g. test that the aggregator correctly formats a sample event, test that a dummy transport writes output to a buffer, etc.). Also test the combined TelemetryService in a simulation context (possibly with a dummy world) to ensure it can start and stop cleanly and publish expected data.
Dependencies: Requires WP1 (the telemetry interface in place) and WP2 (world facade) to be done[41]. The telemetry should interact with world state only through the WorldRuntime interface now, which WP2 ensures. This prevents the telemetry refactor from reaching into removed internals.
Exit Criteria: The telemetry code is split into modules (e.g. telemetry/aggregator.py, telemetry/transport/…, etc.), and the main publisher module is much slimmer, focusing on orchestration if it even remains at all. The simulation loop interacts with telemetry via a single service interface [e.g. one TelemetrySink object](42). We should see improved test outcomes: it should be possible to run the simulation in a mode with telemetry fully disabled or using a stub (to verify the optional behavior), and all tests pass. Additionally, no more ad-hoc thread management scattered in telemetry code – it should all go through clean start/stop hooks.
Work Package 4: Abstract Policy Backends
Objective: Decouple the policy/AI logic from the core simulation by introducing a pluggable backend system for agent decision-making. This allows multiple policy implementations (e.g. basic script vs ML-driven) to coexist and be selected via config[43]. It also ensures the simulation can run without requiring ML libraries unless needed.
Key Activities: - Implement the PolicyBackend interface (from WP1) with at least two concrete backends: a scripted policy backend and a PyTorch-based backend[44]. The scripted backend will be the default, containing simple rule-based or deterministic logic (sufficient for basic gameplay and tests, and does not require torch). The PyTorch backend will be provided under the [ml] extra, using neural network models or RL algorithms as appropriate. - Refactor existing policy logic: move any training-specific utilities (e.g. replay buffers, optimization routines, annealing schedules) out of the core policy.runner into modules that belong either to the PyTorch backend or a shared utility module used by backends[45]. The idea is that the core simulation loop doesn’t need to know these details; it just calls policy_backend.select_actions() or policy_backend.train() and the implementation handles it. - Update the configuration schema to allow selecting which backend to use[46]. For example, policy: "scripted" vs policy: "pytorch" in a config file could toggle the backend. Add validation so that if an ML backend is chosen but the ML extra isn’t installed, it throws a clear error or falls back to default. - Ensure graceful degradation: if the user has not installed the [ml] extra (no PyTorch), the system should automatically default to the scripted backend (or a stub) even if the config wanted the ML one, and log a warning. Write tests to simulate this scenario to confirm no exceptions are raised when torch is missing [this ties back to WP0’s dependency guards](47). Also mark any tests for the ML backend to skip if torch is not present.
Dependencies: Depends on WP1 (protocols defined) and also leverages WP0 (optional dependency handling) to ensure the PyTorch integration is truly optional[48]. It can be done in parallel with WP2 and WP3 to some extent, but best after WP1.
Exit Criteria: The simulation can use different policy backends without code changes – just by config. E.g., one can run a headless simulation using the lightweight scripted AI, or switch to the ML backend if they have [ml] installed, and both work. Tests for policy logic should pass with the default backend and skip appropriately for the ML-specific tests[47]. The policy module should no longer assume torch is present; importing the policy package in a non-ML environment should not error. This means we have achieved a design where ML is an optional plugin, not a hard requirement.
Work Package 5: Decompose Configuration Loader
Objective: Simplify and modularize the configuration system in line with the new subsystem boundaries. The current config loader is a single large module with cross-imports and runtime logic[49]; we will split it so each domain’s config is defined in isolation, improving clarity and reducing import side effects.
Key Activities: - Split the monolithic config definitions into multiple modules grouped by domain[50]. For example, have config/rewards.py for reward-related settings, config/telemetry.py for telemetry settings, config/perturbations.py, config/snapshots.py, etc. Each defines Pydantic models or dataclasses for that section of the config. - Create a light config/loader.py that acts as the orchestrator – it can read the master YAML or JSON config and use the smaller domain-specific models to construct the overall configuration object[51]. This loader handles I/O and delegates to sub-models, rather than containing all schema in one place. - Update references in code to use the new config structure. This might also involve cleaning up how configuration is passed around: ensure that each subsystem (world, policy, telemetry, etc.) only receives the config relevant to it (following the new boundaries). For instance, telemetry code shouldn’t need to know about reward config, etc. - Improve documentation of the configuration schema. Using Pydantic’s Field descriptions or similar, ensure each config field is well documented in code. Possibly auto-generate reference docs from the schema as part of the docs site[52].
Dependencies: It’s best to undertake this after the major subsystems (WP2–WP4) are refactored[53], because those changes will inform how we reorganize the config. E.g., once telemetry is broken out, we know to have a TelemetryConfig model separate from others.
Exit Criteria: The config loader no longer has tangled cross-imports or runtime side effects. Each config model should ideally import only the bare minimum [no more config depending on world or snapshot code at import time](53). We should be able to use parts of the config in isolation (for testing or documentation generation). Documentation is updated to reflect the new config structure, and we target an increase in docstring coverage for config models [e.g. surpass 60% docstring coverage for the config package as a milestone](54). Overall, the configuration system will be easier to extend and less prone to causing import issues.
Work Package 6: Quality and Observability Ratchet
Objective: After restructuring the architecture, devote time to polish quality and extend observability. This phase is about tightening the bolts – fixing lingering lint/type issues, improving docs, and adding nice-to-have telemetry features now that the foundation is solid[55].
Key Activities: - Go through the remaining ruff warnings and mypy ignore flags that might have been added during refactoring, and eliminate them one by one[56]. The goal is to reach a state of zero (or near-zero) lint errors and clean mypy type-check results on the whole codebase. This might involve small refactors or adding type annotations where missing. - Extend the telemetry system with additional adapters or outputs as a real-world test of the new modular design[57]. For instance, implement a Prometheus metrics exporter or a WebSocket streamer for live visualization. This will not only add functionality but also validate that the Telemetry interface is flexible enough to accommodate new plugins without changes. - Update and expand developer documentation and guides. Document the new architecture, the interfaces (WorldRuntime, etc.), how to add a new policy backend or telemetry adapter, etc. Ensure the README and other docs are up-to-date with the post-refactor usage and development practices[58]. Possibly include an architecture diagram and a write-up of the design decisions for transparency. - (Ongoing) Encourage community feedback or contributions on the new structure, using issues or discussions to further identify any rough edges in the API or design that could be smoothed out now, before a 1.0 release.
Dependencies: This is the final phase after all earlier work packages (0–5) are completed[59]. It can run in parallel with late stages of WP5 if needed, but it's essentially the hardening stage once the main refactor is done.
Exit Criteria: The codebase meets the agreed quality thresholds – e.g., no lint errors, 100% type check pass, and a much higher docstring coverage (ideally >80% overall) to ensure long-term maintainability[55]. At least one new telemetry integration (or other extension) is implemented to prove the extensibility of the new architecture in practice[60]. All tests still pass, and performance is not regressed significantly despite the added abstraction (we should run some benchmarks or longer simulations to confirm). At this point, Townlet’s architecture will have been successfully transformed to the target state, setting the stage for a 1.0 release with confidence in its design.
________________________________________
[1] [2] [3] [4] [5] [6] [22] [49] townlet-architectural-analysis.md
<https://github.com/tachyon-beep/townlet/blob/419e640ad0d8b556ec4a80ec4a4e4de747dd5f3f/docs/architecture_review/townlet-architectural-analysis.md>
[7] [8] [9] [10] [11] [12] [14] [15] [16] [17] [18] [19] [20] [21] [23] [24] [25] [26] [27] townlet-target-architecture.md
<https://github.com/tachyon-beep/townlet/blob/419e640ad0d8b556ec4a80ec4a4e4de747dd5f3f/docs/architecture_review/townlet-target-architecture.md>
[13] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] townlet-migration-work-packages.md
<https://github.com/tachyon-beep/townlet/blob/419e640ad0d8b556ec4a80ec4a4e4de747dd5f3f/docs/architecture_review/townlet-migration-work-packages.md>
